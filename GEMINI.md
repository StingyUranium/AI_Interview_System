Overall Project Goal:
To create a multi-agent system for automated, personalized interview preparation and evaluation. using Google ADK
refer their website on how to built agents, subagents, tools etc
https://google.github.io/adk-docs
all models in all 3 agents should be gemini-2.0-flash-exp except the embedding model

Agent 1: CV Processing and Vectorization
Role:
To ingest a candidate's CV and transform it into a structured, queryable format for subsequent agents.

Context Engineering Focus:
Input: Raw CV data (e.g., PDF, DOCX, text).

Goal: Create a vector database of the CV's content.

Key Information to Extract:

Skills (e.g., Python, SQL, AWS, Machine Learning)

Work Experience (Company, Role, Duration, Key Responsibilities/Achievements)

Education (University, Degree, Graduation Year)

Projects (Description, Technologies Used)

Internal Process:

Parsing: Use a RAG (Retrieval-Augmented Generation) pipeline to extract text from the CV. This could involve a pre-trained model to identify different sections (e.g., 'Experience', 'Education').

Chunking: Split the extracted text into meaningful chunks (e.g., one chunk per job, one chunk per project).

Embedding: Use a text embedding model (likely from the Google Adk or a compatible one) to convert each chunk into a vector.

Database Storage: Store these vectors in a vector database (simple in-memory vector store).

Context Prompts for Agent 1 (Conceptual):
"You are a CV parsing and vectorization agent. Your task is to process a given CV, extract key professional details, and create a vector database from its content. The output must be a vector representation of the candidate's skills, experience, and education, suitable for retrieval by another agent."

"When processing the 'Experience' section, pay special attention to quantifiable achievements and technologies mentioned. For the 'Skills' section, categorize them into technical and soft skills where possible."

Agent 2: Interviewer
Role:
To conduct an interview based on the candidate's CV and a specified domain, using the gemini-2.0-flash-exp model.

Context Engineering Focus:
Inputs:

The vector database created by Agent 1.

The specified domain for the interview (e.g., "Software Engineering - Python", "Data Science - Machine Learning").

Goal: Ask a series of relevant, personalized, and domain-specific questions.

Process:

Retrieval: Query the vector database using the interview domain and generic interview prompts to retrieve relevant information from the candidate's CV (e.g., "Find all mentions of 'Python' or 'machine learning' in the CV.").

Question Generation: Use the retrieved information to craft a question.

Example: If the CV mentions a project using TensorFlow, the agent should ask, "I see you worked on a project involving TensorFlow. Can you tell me more about the specific challenges you faced and how you overcame them?"

Interaction Loop: Ask a question, wait for a response, and then generate the next question, either as a follow-up or a new topic, based on the previous answer and the CV.

Context Prompts for Agent 2 (Conceptual):
"You are an interviewer agent using the gemini-2.0-flash-exp model. Your task is to conduct a technical interview for the role of [Domain]. The interview must be personalized based on the candidate's professional background, which you can retrieve from the provided vector database of their CV."

"Begin the interview with a general introduction, then ask a question that combines a specific detail from their CV with a core concept from the [Domain] domain. For example, if they have experience with 'AWS', ask how they would handle a scalability issue for a service they built on AWS."

"Ensure your questions are open-ended and encourage the candidate to elaborate. Avoid simple yes/no questions. After each response, either ask a follow-up question to probe deeper or transition to a new topic based on the remaining CV content."

Agent 3: Evaluator and Feedback Provider
Role:
To assess the candidate's replies, score them, and provide constructive feedback.

Context Engineering Focus:
Inputs:

The candidate's response to a specific question.

The original question asked by Agent 2.

The relevant context from the CV that led to the question.

A scoring rubric or criteria for the given domain.

Goal: Evaluate the answer's correctness and depth, provide a score, and offer specific, actionable feedback for improvement.

Process:

Analysis: Compare the candidate's response against a "golden answer" (which could be pre-defined or generated by a more advanced model based on the domain) and the context of their CV.

Scoring: Assign a score based on a predefined rubric (e.g., a scale of 1-5). The rubric could consider factors like:

Technical accuracy.

Clarity and structure of the explanation.

Relevance to the question.

Demonstration of problem-solving skills.

Feedback Generation: Generate a detailed explanation of why the answer received that score.

Positive Feedback: Highlight what was done well.

Constructive Feedback: Point out areas for improvement. This is where the context engineering is crucial. The feedback should be specific, such as "You correctly identified the algorithm, but you could improve your answer by also mentioning its time complexity and potential edge cases."

Improvement Plan: Suggest concrete steps the candidate can take to improve for next time.

Context Prompts for Agent 3 (Conceptual):
"You are an evaluation and feedback agent. Your task is to assess the quality of an interview response based on the original question, the candidate's CV, and a set of domain-specific criteria. Your output must be a numerical score and a detailed, constructive feedback message."

"For each response, first, analyze its technical accuracy. Then, provide a score from 1 to 5. After scoring, generate a feedback message that starts with what the candidate did well, then identifies one or two specific areas for improvement, and finally, offers actionable advice. For example: 'You correctly identified the use of [technique]. To improve, consider explaining the trade-offs of using this approach versus an alternative like [alternative technique].'"

"The feedback should be encouraging and focus on helping the candidate grow. Avoid overly negative language. Always provide a clear reason for the score given."